{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import test as t\n",
    "\n",
    "t.helloWorld()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Documents\\git\\first-neural-net\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import tarfile\n",
    "from os import getcwd\n",
    "from zipfile import ZipFile\n",
    "from time import sleep\n",
    "\n",
    "print (getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bike-Sharing-Dataset.zip: 287KB [00:01, 278KB/s]                                                                                                                  \n"
     ]
    }
   ],
   "source": [
    "def get_filename (path):\n",
    "    return path.split('/')[-1]\n",
    "\n",
    "def retrieve (url, dest_dir='.', desc='', overwrite=False):\n",
    "\n",
    "    def pbar_updater (block_num, block_size, total_size):\n",
    "        '''ref: https://github.com/tqdm/tqdm#hooks-and-callbacks'''\n",
    "        if pbar.total != total_size:\n",
    "            pbar.total = total_size\n",
    "        pbar.update(block_num * block_size - pbar.n)\n",
    "        '''pbar.n is the current total of bytes downloaded'''\n",
    "        return\n",
    "\n",
    "    filename = get_filename(url)\n",
    "    destination = dest_dir + \"/\" + filename\n",
    "    if not isfile(destination) or overwrite:\n",
    "        with tqdm (unit='B', unit_scale=True, miniters=1) as pbar:\n",
    "            if desc == '': \n",
    "                desc = filename\n",
    "            pbar.set_description(desc)\n",
    "            urlretrieve(url, destination, pbar_updater)\n",
    "            pbar.refresh()\n",
    "    return destination\n",
    "\n",
    "def unzip(source_filename, dest_dir):\n",
    "    '''ref: https://docs.python.org/2/library/zipfile.html#zipfile.ZipFile.extract'''\n",
    "    with ZipFile(source_filename) as zf:\n",
    "        zf.extractall(dest_dir)        \n",
    "    return\n",
    "\n",
    "url_bike_sharing_data = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip'\n",
    "data_dir              = 'data'\n",
    "file_name = retrieve(url       = url_bike_sharing_data,\n",
    "                     dest_dir  = data_dir,\n",
    "                     overwrite = True)\n",
    "\n",
    "unzip (file_name, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bike_share_hour_filename = data_dir + '/hour.csv'\n",
    "df_bike_share_raw = pd.read_csv (bike_share_hour_filename)\n",
    "'''\n",
    "df_hours format:\n",
    "- each row is one hour of data for bike sharing\n",
    "- primary key: instant\n",
    "- other primary keys: dte_day + hr\n",
    "'''\n",
    "\n",
    "\n",
    "print (df_bike_share_raw.head())\n",
    "print (df_bike_share_raw.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_ohe_fields(df_bike_share_data, \n",
    "               categorical_fields=['season', 'weathersit', 'mnth', 'hr', 'weekday', 'holiday']):\n",
    "\n",
    "    #--- one hot encode categorical variables\n",
    "    #--- TODO: explain design decision in documents (i.e. which categorical fields, why ohe?)\n",
    "    for f in categorical_fields:\n",
    "        dummies = pd.get_dummies(df_bike_share_data[f], prefix=f, drop_first=False)\n",
    "        bike_share_data = pd.concat([df_bike_share_data, dummies], axis=1)\n",
    "        \n",
    "    #--- TODO: change axis to AX_COLUMNS\n",
    "    df_bike_share_data = df_bike_share_data.drop(categorical_fields, axis=1)\n",
    "        \n",
    "    return df_bike_share_data\n",
    "        \n",
    "    \n",
    "def trim_unused_fields(df_bike_share_data,\n",
    "                         unused_fields=['instant', 'dteday', 'atemp', 'workingday', 'yr']):\n",
    "    #--- drop fields that network will not use\n",
    "    #--- TODO: explain design decision (i.e. which fields and why)\n",
    "    df_bike_share_data = df_bike_share_data.drop(unused_fields, axis=1)\n",
    "    \n",
    "    return df_bike_share_data\n",
    "\n",
    "df_bike_share_ohe = create_ohe_fields(df_bike_share_raw)\n",
    "df_bike_share_trimmed = trim_unused_fields(df_bike_share_ohe)\n",
    "df_bike_share_ohe.head()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(df_bike_share_trimmed.loc[0, 'temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def rescale_to_z_scores(df_bike_share_data):\n",
    "    \n",
    "    #--- note: DO NOT CALL more than once\n",
    "\n",
    "    #--- refactor continuous varibales to a z-score\n",
    "    #--- z[i] = (x[i] - x_mean) / x_std_dev\n",
    "    continuous_fields = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']\n",
    "\n",
    "    #--- use a dictionary to store mean and std_dev for each field\n",
    "    z_factors = {}\n",
    "\n",
    "    for field in continuous_fields:\n",
    "        df_column = df_bike_share_data[field]\n",
    "        mean      = df_column.mean()\n",
    "        std       = df_column.std()\n",
    "        z_field = 'z' + field\n",
    "        \n",
    "        df_bike_share_data[z_field] = (df_bike_share_data[field] - mean) / std\n",
    "        \n",
    "        z_factors[field] = (mean, std)\n",
    "    \n",
    "    return df_bike_share_data, z_factors\n",
    "\n",
    "df_bike_share_z_scored, z_factors = rescale_to_z_scores(df_bike_share_trimmed)\n",
    "print(df_bike_share_z_scored)\n",
    "print(z_factors['cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bike-Sharing-Dataset.zip: 287KB [00:00, 372KB/s]                                                                                                                  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class BikeShareData:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 url     = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip',\n",
    "                 data_dir='data',\n",
    "                 filename_hour_data = 'hour.csv'):\n",
    "        \n",
    "        self.url                = url\n",
    "        self.data_dir           = data_dir\n",
    "        self.filename_hour_data = filename_hour_data\n",
    "        self.df_bike_share_data = None   #--- pandas dataframe to store bike sharing data\n",
    "        \n",
    "        self.categorical_cols = ['season', 'weathersit', 'mnth', 'hr', 'weekday', 'holiday']\n",
    "        self.unused_cols      = ['instant', 'dteday', 'atemp', 'workingday', 'yr']\n",
    "        self.continuous_cols  = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']\n",
    "        self.z_factors        = {}\n",
    "        self.target_cols      = ['cnt', 'casual', 'registered']\n",
    "        self.target_cols      = ['cnt']\n",
    "\n",
    "        self.ax_column        = 1 #--- constant for axis number of columns in dataframe\n",
    "        \n",
    "        self.days_for_test_data       = 21    \n",
    "        self.days_for_validation_data = 60\n",
    "\n",
    "        return\n",
    "    \n",
    "    def retrieve(self):\n",
    "        \n",
    "        file_name = retrieve(url=self.url, dest_dir=self.data_dir, overwrite=True)\n",
    "        unzip(file_name, self.data_dir)\n",
    "        \n",
    "        #--- create the pandas data frame for the bike share data from the unzip .csv file\n",
    "        self.df_bike_share_data = pd.read_csv(self.data_dir + '/' + self.filename_hour_data)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def preprocess(self):\n",
    "        \n",
    "        self._create_ohe_cols()\n",
    "        self._drop_unused_cols()\n",
    "        self._rescale_cont_cols()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def create_data_sets(self):\n",
    "        \n",
    "        training_set, validation_set = self._create_training_set()\n",
    "        testing_set                  = self._create_testing_set()\n",
    "\n",
    "        return training_set, validation_set, testing_set\n",
    "    \n",
    "    \n",
    "    def _create_ohe_cols(self):\n",
    "\n",
    "        #--- one hot encode (ohe) categorical values\n",
    "        #--- TODO: explain design decision in documents (i.e. which categorical fields, why ohe?)\n",
    "        for col in self.categorical_cols:\n",
    "            dummies = pd.get_dummies(self.df_bike_share_data[col], prefix=col, drop_first=False)\n",
    "            self.df_bike_share_data = pd.concat([self.df_bike_share_data, dummies], axis=self.ax_column)\n",
    "\n",
    "        #--- remove the categorical columns since they are now one-hot-encoded\n",
    "        #--- TODO: change axis to AX_COLUMNS\n",
    "        self.df_bike_share_data = self.df_bike_share_data.drop(self.categorical_cols, axis=self.ax_column)\n",
    "\n",
    "        return \n",
    "\n",
    "    def _drop_unused_cols(self):\n",
    "        #--- drop columns that network will not use\n",
    "        #--- TODO: explain design decision (i.e. which fields and why)\n",
    "        #--- TODO: change axis to AX_COLUMNS\n",
    "\n",
    "        self.df_bike_share_data = self.df_bike_share_data.drop(self.unused_cols, axis=self.ax_column)\n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def _rescale_cont_cols(self):\n",
    "\n",
    "        #--- calling more than once\n",
    "        #--- rescale continuous values to a z-score: z[i] = (x[i] - x_mean) / x_std_dev\n",
    "        #--- use the z_factors dictionary to store mean and std_dev for each col\n",
    "        \n",
    "        for col in self.continuous_cols:\n",
    "            mean    = self.df_bike_share_data[col].mean()\n",
    "            std_dev = self.df_bike_share_data[col].std()\n",
    "            \n",
    "            self.df_bike_share_data[col] = (self.df_bike_share_data[col] - mean) / std_dev\n",
    "            self.z_factors[col]          = (mean, std_dev)\n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def _create_training_set(self):\n",
    "        \n",
    "        #--- keep ALL UP TO the last n days [:-n*24] of data for training the network\n",
    "        df_training_data = self.df_bike_share_data[:-self.days_for_test_data * 24]\n",
    "        \n",
    "        #--- split training data into features and targets\n",
    "        df_features   = df_training_data.drop(self.target_cols, axis=self.ax_column)\n",
    "        df_targets    = df_training_data[self.target_cols]\n",
    "        \n",
    "        #--- split features and targets into sets used for training and validation of the training\n",
    "        idx_validation_data_split = self.days_for_validation_data * 24    \n",
    "        \n",
    "        #--- keep ALL UP TO the last n days [:-n] of features and targets for training data \n",
    "        df_training_features   = df_features[:-idx_validation_data_split]\n",
    "        df_training_targets    = df_targets[:-idx_validation_data_split]\n",
    "        \n",
    "        #--- keep THE LAST n days [-n:] for validation (during training)\n",
    "        df_validation_features = df_features[-idx_validation_data_split:]\n",
    "        df_validation_targets  = df_targets[-idx_validation_data_split:]\n",
    "\n",
    "        #--- create the training and validation sets as tuples of their respective features and targets\n",
    "        training_set           = (df_training_features, df_training_targets)\n",
    "        validation_set         = (df_validation_features, df_validation_targets)\n",
    "        \n",
    "        return training_set, validation_set\n",
    "    \n",
    "    \n",
    "    def _create_testing_set(self):\n",
    "        \n",
    "        #--- keep THE LAST n days [-n*24:] of data for testing the neural network\n",
    "        df_testing_data = self.df_bike_share_data[-self.days_for_test_data * 24:]\n",
    "        \n",
    "        #--- split testing data into features and targets\n",
    "        df_test_features = df_testing_data.drop(self.target_cols, axis=self.ax_column)\n",
    "        df_test_targets  = df_testing_data[self.target_cols]\n",
    "        \n",
    "        #--- create the test set as tuples of the test features and test targets\n",
    "        testing_set = (df_test_features, df_test_targets)\n",
    "        \n",
    "        return testing_set\n",
    "        \n",
    "    \n",
    "\n",
    "data = BikeShareData()\n",
    "data.retrieve()\n",
    "data.preprocess()\n",
    "\n",
    "training, validation, test = data.create_data_sets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15435, 1, 58) (15435, 1, 1)\n",
      "Dense: (58, 128)\n",
      "Sigmoid Activation: (128, 128)\n",
      "Dense: (128, 1)\n",
      "Activation(Default Linear): (1, 1)\n",
      "Epoch 184 of 600 - Loss:3.79815527618\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-e04cbd00a733>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    384\u001b[0m loss_history, validation_loss_history = neural_net.fit(inputs, targets, \n\u001b[0;32m    385\u001b[0m                               \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m600\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m                               validation_data=validation_data)\n\u001b[0m\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-90-e04cbd00a733>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, inputs, targets, epochs, learning_rate, batch_size, verbose, validation_data)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtraining_batches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m                 \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m                 \u001b[0mbatch_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-90-e04cbd00a733>\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, batch_features, batch_labels, learning_rate)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m#--- process each row of features and labels in batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_errors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-90-e04cbd00a733>\u001b[0m in \u001b[0;36mupdate_outputs\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m#--- one forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m#--- update first hidden layer [0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m#--- update all remaining layers [1:]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-90-e04cbd00a733>\u001b[0m in \u001b[0;36mupdate_outputs\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# ... because it will be used to update delta_w in update_errors()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import sleep   # TODO: Delete after debug\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.layers = []\n",
    "        self.training_batches = []\n",
    "        self.lowest_loss = float('Inf')\n",
    "        return\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append (layer)\n",
    "        return\n",
    "    \n",
    "    def compile(self):\n",
    "        prev_layer = None\n",
    "        for layer in self.layers:\n",
    "            layer.compile(prev_layer)\n",
    "            prev_layer = layer\n",
    "        return\n",
    "    \n",
    "    def summary(self):\n",
    "        for layer in self.layers:\n",
    "            layer.summary()\n",
    "        \n",
    "    def update_outputs(self, inputs):\n",
    "\n",
    "        #--- one forward pass\n",
    "        #--- update first hidden layer [0]\n",
    "        self.layers[0].update_outputs(inputs)\n",
    "        \n",
    "        #--- update all remaining layers [1:]\n",
    "        prev_layer = self.layers[0]\n",
    "        for layer in self.layers[1:]:\n",
    "            layer.update_outputs(prev_layer.outputs)\n",
    "            prev_layer = layer\n",
    "        return \n",
    "    \n",
    "    def update_errors(self, labels):\n",
    "\n",
    "        #--- back propagate output errors\n",
    "        #--- output layer is last layer [-1]\n",
    "        output_error = np.array(labels - self.layers[-1].outputs, ndmin=2)\n",
    "        self.layers[-1].update_errors(output_error)\n",
    "        \n",
    "        #--- backpropagate error by iterating the reversed order of layers\n",
    "        prev_layer = self.layers[-1]\n",
    "        for layer in (self.layers[::-1])[1:]:\n",
    "            layer.update_errors(prev_layer.errors)\n",
    "            prev_layer = layer\n",
    "        return\n",
    "\n",
    "    def update_weights(self, learning_rate, batch_size):\n",
    "        for layer in self.layers:\n",
    "            layer.update_weights(learning_rate, batch_size)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def _init_training_batches(self, inputs, targets, batch_size):\n",
    "        num_samples = len(inputs)\n",
    "        self.training_batches=[]\n",
    "        \n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end            = min (start + batch_size, num_samples)\n",
    "            features       = inputs[start:end]\n",
    "            labels         = targets[start:end]\n",
    "            training_batch = (features, targets)\n",
    "            self.training_batches.append(training_batch)\n",
    "        return \n",
    "    \n",
    "    def create_batches(self, inputs, targets, batch_size=32):\n",
    "        num_samples = len(inputs)\n",
    "        batches=[]\n",
    "        \n",
    "        for start in range(0, num_samples, batch_size):\n",
    "            end      = min (start + batch_size, num_samples)\n",
    "            features = inputs[start:end]\n",
    "            labels   = targets[start:end]\n",
    "            batch    = (features, targets)\n",
    "            batches.append(batch)\n",
    "            \n",
    "        return batches\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _init_weight_steps(self):\n",
    "        for layer in self.layers:\n",
    "            layer.init_weight_steps()\n",
    "    \n",
    "    \n",
    "    def predict_on_batch(self, batch_features):\n",
    "        batch_predictions = np.array([])\n",
    "        for features in batch_features:\n",
    "            self.update_outputs(features)\n",
    "            batch_predictions = np.append(batch_predictions, self.layers[-1].outputs)\n",
    "        return batch_predictions\n",
    "    \n",
    "    \n",
    "    def test_on_batch(self, batch_features, batch_labels):\n",
    "        batch_predictions = self.predict_on_batch(batch_features)\n",
    "        sq_errors = [(labels - predictions) ** 2 for labels, predictions in zip(batch_labels, batch_predictions)]\n",
    "        loss = np.mean(sq_errors)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def train_on_batch(self, batch_features, batch_labels, learning_rate):\n",
    "        \n",
    "        #--- len (batch_features) == len (batch_labels)\n",
    "        self._init_weight_steps()\n",
    "        \n",
    "        #--- process each row of features and labels in batch\n",
    "        for features, labels in zip(batch_features, batch_labels):\n",
    "            self.update_outputs(features)\n",
    "            self.update_errors(labels)\n",
    "            \n",
    "        #--- processed all rows, update weights and evaluate model\n",
    "        self.update_weights(learning_rate, batch_size=len(batch_features))\n",
    "        batch_loss = self.test_on_batch(batch_features, batch_labels)\n",
    "        \n",
    "        return batch_loss\n",
    "        \n",
    "    \n",
    "    def fit(self, inputs, targets, epochs, learning_rate, batch_size, verbose=1, validation_data=None):\n",
    "        '''\n",
    "        inputs:  numpy array of shape (num_inputs, num_features)\n",
    "        targets: numpy array of shape (num_inputs, num_labels) i.e. target.shape[0] == inputs.shape[0]\n",
    "        '''\n",
    "        \n",
    "        #--- TODO: create a tool that converts pandas dataframe to numpy array\n",
    "        \n",
    "        training_batches = self.create_batches (inputs, targets, batch_size)\n",
    "        if validation_data is None:\n",
    "            validation_batches = []\n",
    "        else:\n",
    "            validation_inputs, validation_targets = validation_data\n",
    "            validation_batches = self.create_batches (validation_inputs, validation_targets, batch_size)\n",
    "            \n",
    "        loss_history = []\n",
    "        validation_loss_history = []\n",
    "        for e in range(epochs):\n",
    "            #--- process epoch e\n",
    "            batch_losses = np.array([])\n",
    "\n",
    "            for batch_features, batch_labels in training_batches:\n",
    "                batch_loss = self.train_on_batch(batch_features, batch_labels, learning_rate)\n",
    "                batch_losses = np.append(batch_losses, batch_loss)\n",
    "            epoch_loss = np.mean(batch_losses)\n",
    "            loss_history.append(epoch_loss)\n",
    "            \n",
    "            if validation_data is not None:\n",
    "                validation_losses = np.array([])\n",
    "                for validation_batch_features, validation_batch_labels in validation_batches:\n",
    "                    validation_loss = self.test_on_batch(validation_batch_features, validation_batch_labels)\n",
    "                    validation_losses = np.append(validation_losses, validation_loss)\n",
    "                epoch_validation_loss = np.mean(validation_losses)\n",
    "                validation_loss_history.append(epoch_validation_loss)\n",
    "            \n",
    "            if (verbose != 0):\n",
    "                print (\"Epoch \" + str(e + 1) + \" of \" + str (epochs) + \" - Loss:\" + str(epoch_validation_loss), end=\"\\r\")\n",
    "                sleep(0.0005)\n",
    "        return loss_history, validation_loss_history\n",
    "    \n",
    "\n",
    "class TrainingBatch():\n",
    "    \n",
    "    def __init(self):\n",
    "        self.samples = np.array([])\n",
    "            \n",
    "class Layer():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input_size = None\n",
    "        self.output_size = None\n",
    "        self.outputs = None\n",
    "        self.errors = None\n",
    "        self.name = \"Layer\"\n",
    "        return\n",
    "        \n",
    "    def compile(self, prev_layer):\n",
    "        self.input_size = prev_layer.output_size\n",
    "        self.output_size = self.input_size\n",
    "        return\n",
    "    \n",
    "    def update_outputs(self, inputs):\n",
    "        self.outputs = inputs\n",
    "        return\n",
    "    \n",
    "    def update_errors(self, errors):\n",
    "        self.errors = errors\n",
    "        return\n",
    "    \n",
    "    def update_weights(self, learning_rate, batch_size):\n",
    "        pass\n",
    "    \n",
    "    def init_weight_steps(self):\n",
    "        pass\n",
    "    \n",
    "    def summary(self):\n",
    "        print (self.name + \":\", (self.input_size, self.output_size))\n",
    "        \n",
    "        \n",
    "            \n",
    "class Activation(Layer):\n",
    "    '''default activation layer is a linear activation'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        Layer.__init__(self)\n",
    "        self.name = \"Activation(Default Linear)\"\n",
    "        return\n",
    "    \n",
    "    def compile(self, prev_layer):\n",
    "        self.input_size = prev_layer.output_size\n",
    "        self.output_size = self.input_size\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def f(self, x):\n",
    "        #override f in descendants\n",
    "        #linear activation is f(x) = x\n",
    "        return x\n",
    "    \n",
    "    def dfdx(self):\n",
    "        # override dfdx (derivative of f(x) wrt x) in descendents\n",
    "        # derivative of f(x) is f'(x) = 1 \n",
    "        # dfdx() is used in the call to update_errors\n",
    "        return 1\n",
    "    \n",
    "    def update_outputs(self, x):\n",
    "        # linear activation \n",
    "        self.outputs = self.f(x)\n",
    "        return\n",
    "    \n",
    "    def update_errors(self, errors):\n",
    "        self.errors = errors * self.dfdx()\n",
    "        return\n",
    "    \n",
    "    def update_weights(self, learning_rate, batch_size):\n",
    "        # activation layers have no weights to update\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    \n",
    "class Sigmoid(Activation):\n",
    "    \n",
    "    def __init__(self):\n",
    "        Activation.__init__(self)\n",
    "        self.name = \"Sigmoid Activation\"\n",
    "        return\n",
    "    \n",
    "    def f(self, x):\n",
    "        # f(x) = 1 / (1 + exp(x))\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "    def dfdx(self):\n",
    "        # for sigmoid function, f'(x) = f(x) * (1 - f(x))\n",
    "        return self.outputs * (1 - self.outputs)\n",
    "    \n",
    "\n",
    "#--- TODO: fix leaky rectified linear unit activation\n",
    "class LRelu(Activation):\n",
    "    #--- DON't USE YET\n",
    "    \n",
    "    def __init__(self):\n",
    "        Activation.__init__(self)\n",
    "        self.name = \"Leaky ReLU\"\n",
    "        self.threshold = 0.01\n",
    "        self.lrelu_val = None\n",
    "        self.lrelu_grad_val = None\n",
    "        return\n",
    "    \n",
    "    def f(self, x):\n",
    "        #--- x.shape (1, num_outputs)\n",
    "        if self.lrelu_val is None:\n",
    "            self.lrelu_val = np.empty_like(x)\n",
    "        self.lrelu_val[0] = [max (a, self.threshold) for a in x[0]]\n",
    "        return self.lrelu_val\n",
    "    \n",
    "    def dfdx(self):\n",
    "        #--- self.outputs is a vector\n",
    "        #--- self.outputs.shape is (1, num_ouputs)\n",
    "        if self.lrelu_grad_val is None:\n",
    "            self.lrelu_grad_val = np.empty_like(self.outputs)\n",
    "        self.lrelu_grad_val[0] = [1 if a > self.threshold else self.threshold for a in self.outputs[0]]\n",
    "        return self.lrelu_grad_val\n",
    "            \n",
    "    \n",
    "            \n",
    "class Dense(Layer):\n",
    "    \n",
    "    def __init__(self, output_size, input_size=None, weights=None):\n",
    "\n",
    "        Layer.__init__(self)\n",
    "        self.mean = 0.0\n",
    "        self.std_dev = 1.0\n",
    "        self.weights = weights\n",
    "        self.biases = None\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "        self.name = \"Dense\"\n",
    "        self.batch_size = 1\n",
    "\n",
    "        return\n",
    "    \n",
    "        \n",
    "    def _init_weights(self, input_size):\n",
    "        \n",
    "        if self.weights is None:\n",
    "            shape_weights = (input_size, self.output_size)\n",
    "            self.weights = np.random.normal(self.mean, self.std_dev, shape_weights)\n",
    "            self.biases  = np.random.normal(self.mean, self.std_dev, self.output_size)\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def compile(self, prev_layer):\n",
    "\n",
    "        if prev_layer is not None:\n",
    "            self.input_size = prev_layer.output_size\n",
    "        self._init_weights(self.input_size)\n",
    "        return\n",
    "\n",
    "        \n",
    "    def update_outputs(self, x):\n",
    "        \n",
    "        # x.shape should be (1, input_size)\n",
    "        # shape of weights is (input_size, output_size)\n",
    "        # self.outputs = np.dot(x, self.weights) + self.biases\n",
    "        # shape of outputs is (1, output_size)        \n",
    "        # we save x (the layer inputs) to self.inputs...\n",
    "        # ... because it will be used to update delta_w in update_errors()\n",
    "        self.inputs = x    \n",
    "        self.outputs = np.dot(self.inputs, self.weights) \n",
    "        return \n",
    "\n",
    "        \n",
    "    def update_errors(self, errors):\n",
    "        # self.error back propagated to update prev layer\n",
    "        self.delta_w += errors * self.inputs.T\n",
    "        self.errors = np.dot (errors, self.weights.T)\n",
    "\n",
    "        return\n",
    "    \n",
    "    def init_weight_steps(self):\n",
    "        self.delta_w = np.zeros(self.weights.shape)\n",
    "        return\n",
    "    \n",
    "    def update_weights(self, learning_rate, sample_size):\n",
    "        self.weights += learning_rate * self.delta_w / sample_size\n",
    "        return\n",
    "    \n",
    "def pddf_to_nparr(pddf):\n",
    "\n",
    "    rows = np.array(pddf, ndmin=2)\n",
    "    arr = [np.array(row, ndmin=2) for row in rows]\n",
    "    return np.array(arr, ndmin=2)\n",
    "    \n",
    "        \n",
    "'''\n",
    "features = np.array([0.5, 0.1, -0.2], ndmin=2)\n",
    "weights_1 = np.array([[0.5, -0.6], [0.1, -0.2], [0.1, 0.7]])\n",
    "weights_2 = np.array([[0.1], [-0.3]])\n",
    "labels = np.array([0.6], ndmin=2)\n",
    "'''\n",
    "\n",
    "inputs  = pddf_to_nparr(training[0])\n",
    "targets = pddf_to_nparr(training[1])\n",
    "\n",
    "validation_inputs = pddf_to_nparr(validation[0])\n",
    "validation_targets = pddf_to_nparr(validation[1])\n",
    "validation_data = (validation_inputs, validation_targets)\n",
    "\n",
    "print (inputs.shape, targets.shape)\n",
    "\n",
    "neural_net = NeuralNetwork()\n",
    "neural_net.add(Dense(128, input_size=inputs.shape[2]))\n",
    "neural_net.add(Sigmoid())\n",
    "\n",
    "neural_net.add(Dense(1))\n",
    "neural_net.add(Activation())\n",
    "\n",
    "neural_net.compile()\n",
    "neural_net.summary()\n",
    "loss_history, validation_loss_history = neural_net.fit(inputs, targets, \n",
    "                              learning_rate=0.1, epochs=600, batch_size=128, \n",
    "                              validation_data=validation_data)\n",
    "        \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "plt.plot(loss_history, label='Training loss')\n",
    "plt.plot(validation_loss_history, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.ylim(ymax=0.5)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in a:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4]\n",
    "print (a[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_size = 3\n",
    "layer_size = 4\n",
    "mean = 0.0\n",
    "std_dev = 0.5\n",
    "shape_weights = (input_size, layer_size)\n",
    "weights = np.random.normal(mean, std_dev, shape_weights)\n",
    "print (weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print (training[0].head())\n",
    "print (training[1].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = pd.Series([1, 2])\n",
    "b = pd.Series([3, 4])\n",
    "c = pd.concat([a, b])\n",
    "print (c)\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(np.arange(12).reshape(3,4),\n",
    "                      columns=['A', 'B', 'C', 'D'])\n",
    "\n",
    "def foo (df):\n",
    "    col = df['A']\n",
    "    col = col - 1\n",
    "    df ['A'] = col\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "foo (df)\n",
    "print (df)\n",
    "print (df.loc[:, 'A':'D'])\n",
    "print (df.loc[:, 'A'])\n",
    "print (df.loc[:, 'A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "i = np.array([0.5, 0.1, -0.2], ndmin=2)\n",
    "print (i.shape, i)\n",
    "w = np.array([[0.5, -0.6], [0.1, -0.2], [0.1, 0.7]])\n",
    "out = np.dot(i, w)\n",
    "print (out.T.shape, out.T)\n",
    "0.00160809  * 0.5\n",
    "0.00333551 * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def f():\n",
    "    pass\n",
    "\n",
    "def a():\n",
    "    print ('a')\n",
    "    f()\n",
    "    return\n",
    "\n",
    "a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "test = (1, 2)\n",
    "a, b = test\n",
    "print (b)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
